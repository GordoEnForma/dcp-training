{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.applications import InceptionV3, Xception, ResNet152V2, InceptionResNetV2, DenseNet201, ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model= InceptionV3(include_top=False, weights=\"imagenet\", input_shape=(299,299,3))\n",
    "base_model= InceptionResNetV2(include_top=False, weights=\"imagenet\", input_shape=(299,299,3))\n",
    "# base_model= DenseNet201(include_top=False, weights=\"imagenet\", input_shape=(299,299,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # nb_classes = 7\n",
    "# model = Sequential()\n",
    "# model.add(base_model)\n",
    "# model.add(GlobalAveragePooling2D())\n",
    "# model.add(Dense(7, activation='relu'))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model= Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "# model.add(GlobalAveragePooling2D())\n",
    "model.add(Dropout(0.40))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "data=pd.read_csv(\"HAM10000_metadata_kaggle.csv\")\n",
    "# Quitar duplicados\n",
    "data.drop_duplicates(subset='lesion_id', keep=\"first\")\n",
    "data['image_full_name']=data['image_id']+'.jpg'\n",
    "X=data[['image_full_name','dx','lesion_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Y=X.pop('dx').to_frame()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=42)\n",
    "X_train,X_val,y_train,y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.concat([X_train,y_train],axis=1)\n",
    "val=pd.concat([X_val,y_val],axis=1)\n",
    "test=pd.concat([X_test,y_test],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder= LabelEncoder()\n",
    "encoder.fit(val['dx'])\n",
    "name_as_indexes_train=encoder.transform(val['dx']) \n",
    "val['label']=name_as_indexes_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=LabelEncoder()\n",
    "encoder.fit(test['dx'])\n",
    "name_as_indexes_test=encoder.transform(test['dx']) \n",
    "test['label']=name_as_indexes_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "train_generator = ImageDataGenerator(rescale = 1./255,\n",
    "                                     rotation_range=10,  \n",
    "                                     zoom_range = 0.1, \n",
    "                                     width_shift_range=0.0,  height_shift_range=0.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= train_generator.flow_from_dataframe(dataframe=train,x_col=\"image_full_name\",y_col=\"dx\",\n",
    "                                                batch_size=64,directory=\"ISIC2018_Task3_Training_Input\",\n",
    "                                                shuffle=True,class_mode=\"categorical\",target_size=(299,299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.class_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_generator=ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data= test_generator.flow_from_dataframe(dataframe=test,x_col=\"image_full_name\",y_col=\"dx\",\n",
    "                                              directory=\"ISIC2018_Task3_Training_Input\",\n",
    "                                              shuffle=False,batch_size=1,class_mode=None,target_size=(299,299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data=test_generator.flow_from_dataframe(dataframe=val,x_col=\"image_full_name\",y_col=\"dx\",\n",
    "                                            directory=\"ISIC2018_Task3_Training_Input\",\n",
    "                                            batch_size=64,shuffle=False,class_mode=\"categorical\",target_size=(299,299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "learning_control = ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=1, factor=.5, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "# Save the model with best weights\n",
    "checkpointer = ModelCheckpoint('modelazo.hdf5', verbose=1,save_best_only=True, monitor='val_accuracy',mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=20, verbose=1, mode='max',restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = optimizers.Adam(lr=0.0001)\n",
    "model.compile(optimizer=adam, loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "history = model.fit(train_data,\n",
    "                    steps_per_epoch=train_data.samples//train_data.batch_size,\n",
    "                    validation_data=val_data,\n",
    "                    verbose=1,\n",
    "                    validation_steps=val_data.samples//val_data.batch_size,\n",
    "                    epochs=100, callbacks=[learning_control, checkpointer, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('modelazo.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.reset()\n",
    "predictions = model.predict_generator(test_data, steps=test_data.samples/test_data.batch_size,verbose=1)\n",
    "y_pred= np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "cm= confusion_matrix(name_as_indexes_test,y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para graficar la historia\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "EPOCAS = 43\n",
    "\n",
    "def graficar(h):\n",
    "    LOSS = 0\n",
    "    ACCURACY = 1\n",
    "    entrenamiento = np.zeros((2, EPOCAS))\n",
    "    prueba = np.zeros((2, EPOCAS))\n",
    "    entrenamiento[LOSS] = h.history['loss']  # loss de entrenamiento\n",
    "    prueba[LOSS] = h.history['val_loss']    # loss de validación\n",
    "    entrenamiento[ACCURACY] = h.history['accuracy']  # acc de entrenamiento\n",
    "    prueba[ACCURACY] = h.history['val_accuracy']  # acc de validación\n",
    "    epocas = range(1, EPOCAS+1)\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    for i, label in zip((LOSS, ACCURACY), ('perdida', 'exactitud')):\n",
    "        axs[i].plot(epocas, entrenamiento[i], 'b-', label='Entrenamiento '+label)\n",
    "        axs[i].plot(epocas, prueba[i], 'y-', label='Prueba '+label)\n",
    "        axs[i].set_title('Entrenamiento y prueba ' + label)\n",
    "        axs[i].set_xlabel('Epocas')\n",
    "        axs[i].set_ylabel(label)\n",
    "        axs[i].legend()\n",
    "    plt.show()\n",
    "\n",
    "graficar(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validación Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "# model.add(GlobalAveragePooling2D())\n",
    "model.add(Dropout(0.40))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "adam = optimizers.Adam(lr=0.0001)\n",
    "model.compile(optimizer=adam, loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "history = model.fit(train_data,\n",
    "                    steps_per_epoch=train_data.samples//train_data.batch_size,\n",
    "                    validation_data=val_data,\n",
    "                    verbose=1,\n",
    "                    validation_steps=val_data.samples//val_data.batch_size,\n",
    "                    epochs=100, callbacks=[learning_control, checkpointer, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=2, shuffle=True)\n",
    "for entr, prue in kfold.split(X, np.zeros(shape=(X.shape[0], 1))):\n",
    "    X.to_numpy()[entr]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cross = X.to_numpy()\n",
    "Y_cross = Y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cross = data.to_numpy()\n",
    "# data_cross = data_cross.reshape(data_cross.shape[0], 299, 299, 3)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cross[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Definir una función que crea y compila el modelo\n",
    "def create_model():\n",
    "    modelazo = Sequential()\n",
    "    # Agregar capas al modelo\n",
    "    modelazo.add(model)\n",
    "    modelazo.compile(optimizer='adam',\n",
    "                     loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return modelazo\n",
    "\n",
    "# Crear un clasificador de Keras\n",
    "modelCross = KerasClassifier(build_fn=create_model, epochs=1, batch_size=64)\n",
    "\n",
    "# Obtener todos los datos de entrenamiento\n",
    "# X_cross_train = []\n",
    "# y_cross_train = []\n",
    "# for i in range(len(train_data)):\n",
    "#     batch = next(train_data)\n",
    "#     X_batch = batch[0]\n",
    "#     y_batch = batch[1]\n",
    "#     X_cross_train.append(X_batch)\n",
    "#     y_cross_train.append(y_batch)\n",
    "# X_cross_train = np.concatenate(X_cross_train)\n",
    "# y_cross_train = np.concatenate(y_cross_train)\n",
    "\n",
    "# Realizar la validación cruzada\n",
    "scores = cross_val_score(modelCross, X=X_train, y=y_train, cv=2)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
